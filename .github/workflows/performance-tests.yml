name: Weekly Performance Tests

on:
  schedule:
    - cron: '0 17 * * 1'  # Every Monday at 9am PST
  workflow_dispatch:  # Manual trigger for testing

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          pip install requests
      
      - name: Download JMeter
        run: |
          wget -q https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.6.3.tgz
          tar -xzf apache-jmeter-5.6.3.tgz
          echo "$(pwd)/apache-jmeter-5.6.3/bin" >> $GITHUB_PATH
      
      - name: Create results directories
        run: |
          mkdir -p results logs screenshots
      
      - name: Run Smoke Test
        env:
          INFLUX_URL: ${{ secrets.INFLUX_URL }}
          INFLUX_TOKEN: ${{ secrets.INFLUX_TOKEN }}
          INFLUX_ORG: ${{ secrets.INFLUX_ORG }}
          INFLUX_BUCKET: ${{ secrets.INFLUX_BUCKET }}
        run: |
          jmeter -n -t performance_tests.jmx \
            -JtestRunSelection='Smoke' \
            -JINFLUX_URL="${INFLUX_URL}" \
            -JINFLUX_TOKEN="${INFLUX_TOKEN}" \
            -JINFLUX_ORG="${INFLUX_ORG}" \
            -JINFLUX_BUCKET="${INFLUX_BUCKET}" \
            -l results/smoke_results.jtl \
            -j logs/smoke.log
      
      - name: Wait for Smoke data to settle
        run: sleep 30
      
      - name: Run Load Test
        env:
          INFLUX_URL: ${{ secrets.INFLUX_URL }}
          INFLUX_TOKEN: ${{ secrets.INFLUX_TOKEN }}
          INFLUX_ORG: ${{ secrets.INFLUX_ORG }}
          INFLUX_BUCKET: ${{ secrets.INFLUX_BUCKET }}
        run: |
          jmeter -n -t performance_tests.jmx \
            -JtestRunSelection='Load' \
            -JINFLUX_URL="${INFLUX_URL}" \
            -JINFLUX_TOKEN="${INFLUX_TOKEN}" \
            -JINFLUX_ORG="${INFLUX_ORG}" \
            -JINFLUX_BUCKET="${INFLUX_BUCKET}" \
            -l results/load_results.jtl \
            -j logs/load.log
      
      - name: Wait for Load data to settle
        run: sleep 30
      
      - name: Run Stress Test
        env:
          INFLUX_URL: ${{ secrets.INFLUX_URL }}
          INFLUX_TOKEN: ${{ secrets.INFLUX_TOKEN }}
          INFLUX_ORG: ${{ secrets.INFLUX_ORG }}
          INFLUX_BUCKET: ${{ secrets.INFLUX_BUCKET }}
        run: |
          jmeter -n -t performance_tests.jmx \
            -JtestRunSelection='Stress' \
            -JINFLUX_URL="${INFLUX_URL}" \
            -JINFLUX_TOKEN="${INFLUX_TOKEN}" \
            -JINFLUX_ORG="${INFLUX_ORG}" \
            -JINFLUX_BUCKET="${INFLUX_BUCKET}" \
            -l results/stress_results.jtl \
            -j logs/stress.log
      
      - name: Wait for Stress data to settle
        run: sleep 30
      
      - name: Run Spike Test
        env:
          INFLUX_URL: ${{ secrets.INFLUX_URL }}
          INFLUX_TOKEN: ${{ secrets.INFLUX_TOKEN }}
          INFLUX_ORG: ${{ secrets.INFLUX_ORG }}
          INFLUX_BUCKET: ${{ secrets.INFLUX_BUCKET }}
        run: |
          jmeter -n -t performance_tests.jmx \
            -JtestRunSelection='Spike' \
            -JINFLUX_URL="${INFLUX_URL}" \
            -JINFLUX_TOKEN="${INFLUX_TOKEN}" \
            -JINFLUX_ORG="${INFLUX_ORG}" \
            -JINFLUX_BUCKET="${INFLUX_BUCKET}" \
            -l results/spike_results.jtl \
            -j logs/spike.log
      
      - name: Wait for Spike data to settle
        run: sleep 30
      
      - name: Parse test results and generate stats
        id: parse_results
        run: |
          python3 << 'EOF'
          import csv
          import json
          import os
          from datetime import datetime
          
          def parse_jtl(filepath, test_type):
              """Parse JMeter file and extract stats"""
              try:
                  with open(filepath, 'r') as f:
                      reader = csv.DictReader(f)
                      rows = list(reader)
                  
                  if not rows:
                      return None
                  
                  # Calculate stats
                  latencies = [int(row['elapsed']) for row in rows]
                  success_count = sum(1 for row in rows if row['success'] == 'true')
                  total_count = len(rows)
                  
                  latencies_sorted = sorted(latencies)
                  p50_index = int(len(latencies_sorted) * 0.50)
                  p90_index = int(len(latencies_sorted) * 0.90)
                  p95_index = int(len(latencies_sorted) * 0.95)
                  p99_index = int(len(latencies_sorted) * 0.99)
                  
                  stats = {
                      'test_type': test_type,
                      'total_requests': total_count,
                      'success_rate': round((success_count / total_count) * 100, 2),
                      'avg_response_time': round(sum(latencies) / len(latencies), 2),
                      'min_response_time': min(latencies),
                      'max_response_time': max(latencies),
                      'p50_response_time': latencies_sorted[p50_index],
                      'p90_response_time': latencies_sorted[p90_index],
                      'p95_response_time': latencies_sorted[p95_index],
                      'p99_response_time': latencies_sorted[p99_index],
                      'error_count': total_count - success_count
                  }
                  
                  return stats
              except Exception as e:
                  print(f"Error parsing {filepath}: {e}")
                  return None
          
          # Parse all test results
          results = {}
          for test_type in ['smoke', 'load', 'stress', 'spike']:
              filepath = f'results/{test_type}_results.jtl'
              if os.path.exists(filepath):
                  stats = parse_jtl(filepath, test_type.capitalize())
                  if stats:
                      results[test_type] = stats
          
          # Save to file for email step
          with open('test_results_summary.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          smoke_success = results.get('smoke', {}).get('success_rate', 0)
          load_success = results.get('load', {}).get('success_rate', 0)
          stress_success = results.get('stress', {}).get('success_rate', 0)
          spike_success = results.get('spike', {}).get('success_rate', 0)
          
          overall_success = (smoke_success + load_success + stress_success + spike_success) / 4
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"overall_success={overall_success:.2f}\n")
              f.write(f"smoke_success={smoke_success:.2f}\n")
          
          EOF
      
      - name: Capture Grafana Screenshots
        env:
          GRAFANA_TOKEN: ${{ secrets.GRAFANA_API_KEY }}
          GRAFANA_RENDER_URL: ${{ secrets.GRAFANA_RENDER_URL }}
        run: |
          python3 << 'EOF'
          import os
          import requests
          import time
          
          def capture_screenshot(url, token, test_type, output_path):
              
              params = {
                  'orgId': '1',
                  'var-testType': test_type,
                  'from': 'now-30m',
                  'to': 'now',
                  'width': 1000,
                  'height': 1300,
                  'timeout': 60,
                  'theme': 'dark'
              }
              
              headers = {
                  'Authorization': f'Bearer {token}',
                  'Accept': 'image/png'
              }
              
              print(f"Capturing {test_type} dashboard")
              
              response = requests.get(url, params=params, headers=headers, timeout=120)
              
              if response.status_code == 200:
                  with open(output_path, 'wb') as f:
                      f.write(response.content)
                  print(f"SUCCESS - Saved to {output_path}")
                  return True
              else:
                  print(f"FAILED - HTTP {response.status_code}")
                  print(f"Response: {response.text[:500]}")
                  return False
          
          url = os.environ.get('GRAFANA_RENDER_URL')
          token = os.environ.get('GRAFANA_TOKEN')
          
          for test_type in ['Smoke', 'Load', 'Stress', 'Spike']:
              output_path = f'screenshots/{test_type.lower()}_dashboard.png'
              capture_screenshot(url, token, test_type, output_path)
              time.sleep(3)
          
          EOF
      
      - name: Generate Plain Text Email
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime
          
          # Load test results
          try:
              with open('test_results_summary.json', 'r') as f:
                  results = json.load(f)
          except FileNotFoundError:
              print("Summary file not found")
              exit(1)
              
          # Overall stats calculation
          overall_success = sum(r.get('success_rate', 0) for r in results.values()) / len(results) if results else 0
          total_requests = sum(r.get('total_requests', 0) for r in results.values())
          total_errors = sum(r.get('error_count', 0) for r in results.values())
          
          # Generate Text Body
          text = "Weekly Performance Test Report\n"
          text += f"Automated test run completed on {datetime.now().strftime('%B %d, %Y at %H:%M UTC')}\n\n"
          
          text += f"Overall Success Rate: {overall_success:.1f}%\n"
          text += f"Total Requests: {total_requests:,}\n"
          text += f"Total Errors: {total_errors}\n\n"
          
          # Individual test results
          for test_key, stats in results.items():
              text += f"{stats['test_type'].upper()} TEST RESULTS:\n\n"
              text += f"Success Rate: {stats['success_rate']}%\n"
              text += f"Total Requests: {stats['total_requests']:,}\n"
              text += f"Errors: {stats['error_count']}\n"
              text += f"Avg Response: {stats['avg_response_time']:.0f}ms\n"
              text += f"p50: {stats['p50_response_time']}ms\n"
              text += f"p90: {stats['p90_response_time']}ms\n"
              text += f"p95: {stats['p95_response_time']}ms\n"
              text += f"p99: {stats['p99_response_time']}ms\n"
              text += f"Min: {stats['min_response_time']}ms\n"
              text += f"Max: {stats['max_response_time']}ms\n\n"

          text += "--------------------------------------\n"
          text += "Logs and screenshots are attached as a zip file."
          
          # Save Text email
          with open('email_body.txt', 'w') as f:
              f.write(text)
          
          print("Email generated successfully")
          EOF

      - name: Zip Test Artifacts
        run: |
          zip -r performance_results.zip results/ logs/ screenshots/ test_results_summary.json email_body.txt

      - name: Send Email
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "Performance Test Report - ${{ steps.parse_results.outputs.overall_success }}% Success"
          to: ${{ secrets.EMAIL_TO }}
          from: Performance Testing Bot <${{ secrets.EMAIL_USERNAME }}>
          body: file://email_body.txt
          attachments: performance_results.zip
          content_type: text/plain
          convert_markdown: false
          priority: normal

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            performance_results.zip
            email_body.txt
          retention-days: 7